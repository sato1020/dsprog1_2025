{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "eae7a93d",
   "metadata": {},
   "source": [
    "## 最終課題"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "05cbb9b1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- スタート: https://www.musashino-u.ac.jp/ ---\n",
      "--- 対象: www.musashino-u.ac.jp ---\n",
      "https://www.musashino-u.ac.jp/\n",
      "  Title: 武蔵野大学\n",
      "https://www.musashino-u.ac.jp/research/alignment/collaborativeresearch.html\n",
      "  Title: 連携研究の取り組み | 研究 | 武蔵野大学\n",
      "https://www.musashino-u.ac.jp/happiness_creators/\n",
      "  Title: 輝け！武蔵野大生 Happiness Creators 一覧 | 武蔵野大学\n",
      "https://www.musashino-u.ac.jp/happiness_creators/no025.html\n",
      "  Title: 5歳だった私はいま―３.11の支援を次の支援へ | 武蔵野大学\n",
      "https://www.musashino-u.ac.jp/business.html\n",
      "  Title: 企業・研究者の方 | 武蔵野大学\n",
      "https://www.musashino-u.ac.jp/research/laboratory/pharmacy/\n",
      "  Title: 薬学研究所 | 研究 | 武蔵野大学\n",
      "https://www.musashino-u.ac.jp/research/laboratory/pharmacy/lab/dotai.html\n",
      "  Title: 薬物動態学研究室 | 研究 | 武蔵野大学\n",
      "https://www.musashino-u.ac.jp/research/ethics/\n",
      "\n",
      "--- 中断 ---\n",
      "\n",
      "--- 結果 ---\n",
      "合計ページ数: 7\n",
      "--- 取得した辞書型変数 (scraped_data) の内容 ---\n",
      "URL: https://www.musashino-u.ac.jp/\n",
      "Title: 武蔵野大学\n",
      "\n",
      "URL: https://www.musashino-u.ac.jp/research/alignment/collaborativeresearch.html\n",
      "Title: 連携研究の取り組み | 研究 | 武蔵野大学\n",
      "\n",
      "URL: https://www.musashino-u.ac.jp/happiness_creators/\n",
      "Title: 輝け！武蔵野大生 Happiness Creators 一覧 | 武蔵野大学\n",
      "\n",
      "URL: https://www.musashino-u.ac.jp/happiness_creators/no025.html\n",
      "Title: 5歳だった私はいま―３.11の支援を次の支援へ | 武蔵野大学\n",
      "\n",
      "URL: https://www.musashino-u.ac.jp/business.html\n",
      "Title: 企業・研究者の方 | 武蔵野大学\n",
      "\n",
      "URL: https://www.musashino-u.ac.jp/research/laboratory/pharmacy/\n",
      "Title: 薬学研究所 | 研究 | 武蔵野大学\n",
      "\n",
      "URL: https://www.musashino-u.ac.jp/research/laboratory/pharmacy/lab/dotai.html\n",
      "Title: 薬物動態学研究室 | 研究 | 武蔵野大学\n",
      "\n",
      "{'https://www.musashino-u.ac.jp/': '武蔵野大学', 'https://www.musashino-u.ac.jp/research/alignment/collaborativeresearch.html': '連携研究の取り組み | 研究 | 武蔵野大学', 'https://www.musashino-u.ac.jp/happiness_creators/': '輝け！武蔵野大生 Happiness Creators 一覧 | 武蔵野大学', 'https://www.musashino-u.ac.jp/happiness_creators/no025.html': '5歳だった私はいま―３.11の支援を次の支援へ | 武蔵野大学', 'https://www.musashino-u.ac.jp/business.html': '企業・研究者の方 | 武蔵野大学', 'https://www.musashino-u.ac.jp/research/laboratory/pharmacy/': '薬学研究所 | 研究 | 武蔵野大学', 'https://www.musashino-u.ac.jp/research/laboratory/pharmacy/lab/dotai.html': '薬物動態学研究室 | 研究 | 武蔵野大学'}\n"
     ]
    }
   ],
   "source": [
    "#webスクレイピングに最低限必要なライブラリをインポート\n",
    "import requests\n",
    "import time\n",
    "from bs4 import BeautifulSoup\n",
    "from urllib.parse import urljoin, urlparse\n",
    "\n",
    "# --- 設定 ---\n",
    "#最初に参照するURLを設定する\n",
    "START_URL = \"https://www.musashino-u.ac.jp/\"\n",
    "#対象とするドメイン名を取得する\n",
    "BASE_NETLOC = urlparse(START_URL).netloc\n",
    "\n",
    "# --- データ構造 ---\n",
    "# 取得した結果を格納する辞書 {URL: <title>}として表示する\n",
    "scraped_data = {}\n",
    "\n",
    "# 巡回キュー (これから確認するURLの集合)\n",
    "to_visit = {START_URL}\n",
    "\n",
    "# 既に確認済みのURLの集合\n",
    "visited = set()\n",
    "\n",
    "# --- 関数: ページの情報を取得し、次のリンクを見つける ---\n",
    "def crawl_page(url, scraped_data, to_visit, visited):\n",
    "    \"\"\"単一のURLを処理し、タイトルを取得し、新しいリンクをto_visitに追加する\"\"\"\n",
    "    \n",
    "    # 既に処理済みのURLはスキップできるようにする\n",
    "    if url in visited:\n",
    "        return\n",
    "\n",
    "    print(f\"{url}\")\n",
    "    visited.add(url)\n",
    "    time.sleep(1) # サーバーへの負荷軽減をするために1秒待機させる\n",
    "\n",
    "    #tryを使用することで、エラーが発生した場合にプログラムがクラッシュしないようにする\n",
    "    #→exceptブロックでエラーメッセージを表示し、関数を終了する\n",
    "    try:\n",
    "        # ページのHTMLを取得 \n",
    "        #サーバーから応答が返ってくるまでプログラムが無限に待機しないように、タイムアウトを10秒に設定する\n",
    "        response = requests.get(url, timeout=10)\n",
    "        # 成功ステータスのみ処理\n",
    "        response.raise_for_status() \n",
    "        #ページのエンコーディングを自動で判別して設定できるようにする\n",
    "        response.encoding = response.apparent_encoding \n",
    "    except requests.RequestException as e:\n",
    "        print(f\"  [エラー]: {e}\")\n",
    "        return\n",
    "\n",
    "    # 取得したHTMLをBeautifulSoupを使い解析できる形に変換して解析\n",
    "    soup = BeautifulSoup(response.text, 'html.parser')\n",
    "    \n",
    "    # --- <title>の抽出と辞書への格納 ---\n",
    "    title_tag = soup.find('title')\n",
    "    title = title_tag.text.strip() if title_tag else \"タイトルなし\"\n",
    "    \n",
    "    # 辞書に {URL: <title>} の形式で格納\n",
    "    scraped_data[url] = title\n",
    "    print(f\"  Title: {title}\")\n",
    "\n",
    "    # --- リンクの発見 ---\n",
    "    for link in soup.find_all('a', href=True):\n",
    "        href = link['href']\n",
    "        \n",
    "        # 相対パスをURLに変換し、フラグメント（#）を除去\n",
    "        absolute_url = urljoin(url, href).split('#')[0]\n",
    "        \n",
    "        # URLを解析してドメイン情報をチェック\n",
    "        parsed_url = urlparse(absolute_url)\n",
    "        \n",
    "        #HTMLから抽出したリンクがすでに追加されていないかを確認し、追加されていた場合はスキップする\n",
    "        #追加されていない場合はto_visitに追加する\n",
    "        #parsed_url.netloc == BASE_NETLOCは、リンクが対象ドメイン内であることを確認する\n",
    "        if (parsed_url.netloc == BASE_NETLOC and\n",
    "            #一度参照したurlをまた対象にしてしまった際に無限ループなってしまうことを防ぐために、visitedとto_visitの両方に存在しないURLのみを追加する\n",
    "            absolute_url not in visited and\n",
    "            absolute_url not in to_visit and\n",
    "            #httpまたはhttpsスキームのURLのみを対象とする\n",
    "            parsed_url.scheme in ('http', 'https')):\n",
    "            \n",
    "            to_visit.add(absolute_url)\n",
    "            \n",
    "# --- メイン実行部 ---\n",
    "print(f\"--- スタート: {START_URL} ---\")\n",
    "print(f\"--- 対象: {BASE_NETLOC} ---\")\n",
    "\n",
    "try:\n",
    "    while to_visit:\n",
    "        # to_visitから次のURLを取得\n",
    "        current_url = to_visit.pop()\n",
    "        \n",
    "        # ページの巡回とリンクの探索を実行\n",
    "        crawl_page(current_url, scraped_data, to_visit, visited)\n",
    "\n",
    "except KeyboardInterrupt:\n",
    "    print(\"\\n--- 中断 ---\")\n",
    "    \n",
    "finally:\n",
    "    # --- 結果の出力 ---\n",
    "    print(\"\\n--- 結果 ---\")\n",
    "    print(f\"合計ページ数: {len(scraped_data)}\")\n",
    "    print(\"--- 取得した辞書型変数 (scraped_data) の内容 ---\")\n",
    "    \n",
    "    # 取得した辞書を整形して表示\n",
    "    for url, title in scraped_data.items():\n",
    "        print(f\"URL: {url}\")\n",
    "        print(f\"Title: {title}\\n\")\n",
    "    \n",
    "    # 辞書変数を表示\n",
    "    print(scraped_data)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
